#!/bin/bash

# ============================================================
# Multi-Agent Model Configuration
# Auto-generated by setup.sh v3.1
# ============================================================
#
# This file defines model mappings for each agent role based on
# the detected MCP environment. Models are selected to optimize
# cost/performance balance for each role.
#
# Roles:
#   - ORCHESTRATOR: Planning, code generation (High performance)
#   - ANALYST: Large-scale analysis, research (Large context)
#   - EXECUTOR: Command execution, fast response (Lightweight)
#
# ============================================================

# Environment Detection (set by setup.sh)
export MCP_WORKFLOW_TYPE="${MCP_WORKFLOW_TYPE:-standalone}"

# ============================================================
# Model Definitions by Provider
# ============================================================

# Claude Models (Anthropic) - 2025/2026
declare -A CLAUDE_MODELS=(
    ["opus"]="claude-opus-4-5-20251101"      # $5/$25 - Highest capability
    ["sonnet"]="claude-sonnet-4-5-20241022"  # $3/$15 - Balanced, best coding
    ["haiku"]="claude-haiku-4-5-20241022"    # $1/$5  - Fast, cost-effective
)

# Gemini Models (Google) - 2025/2026
declare -A GEMINI_MODELS=(
    ["3-pro"]="gemini-3-pro"          # Premium - Deep reasoning, 1M context
    ["3-flash"]="gemini-3-flash"      # $0.50/$3 - Fast, Pro-grade reasoning
    ["2.5-pro"]="gemini-2.5-pro"      # Legacy - Stable, large context
    ["2.5-flash"]="gemini-2.5-flash"  # Legacy - Cost-effective
)

# OpenAI/Codex Models - 2025/2026
declare -A OPENAI_MODELS=(
    ["gpt5-codex"]="gpt-5.2-codex"    # Flagship coding agent
    ["gpt5-codex-mini"]="gpt-5.1-codex-mini"  # Lightweight coding
    ["gpt4.1"]="gpt-4.1"              # General purpose
    ["gpt4.1-mini"]="gpt-4.1-mini"    # Cost-effective
    ["gpt4o"]="gpt-4o"                # Multimodal
)

# ============================================================
# Role-based Model Mappings by Workflow Type
# ============================================================

configure_models_for_workflow() {
    local workflow="$1"

    case "$workflow" in
        # --------------------------------------------------------
        # Claude-Only: Use Claude models for all roles
        # --------------------------------------------------------
        "claude-only")
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[opus]}"
            export MODEL_ORCHESTRATOR_FALLBACK="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ANALYST="${CLAUDE_MODELS[sonnet]}"      # Analyst role
            export MODEL_ANALYST_FALLBACK="${CLAUDE_MODELS[haiku]}"
            export MODEL_EXECUTOR="${CLAUDE_MODELS[haiku]}"      # Fast execution
            export MODEL_EXECUTOR_FALLBACK="${CLAUDE_MODELS[haiku]}"

            export MODEL_PROVIDER_ORCHESTRATOR="claude"
            export MODEL_PROVIDER_ANALYST="claude"
            export MODEL_PROVIDER_EXECUTOR="claude"
            ;;

        # --------------------------------------------------------
        # Claude + Gemini: Gemini for analysis, Claude for others
        # --------------------------------------------------------
        "claude-gemini")
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[opus]}"
            export MODEL_ORCHESTRATOR_FALLBACK="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ANALYST="${GEMINI_MODELS[3-pro]}"       # Gemini for large analysis
            export MODEL_ANALYST_FALLBACK="${GEMINI_MODELS[3-flash]}"
            export MODEL_EXECUTOR="${CLAUDE_MODELS[haiku]}"      # Claude Haiku for execution
            export MODEL_EXECUTOR_FALLBACK="${CLAUDE_MODELS[haiku]}"

            export MODEL_PROVIDER_ORCHESTRATOR="claude"
            export MODEL_PROVIDER_ANALYST="gemini"
            export MODEL_PROVIDER_EXECUTOR="claude"
            ;;

        # --------------------------------------------------------
        # Claude + Codex: Codex for execution, Claude for others
        # --------------------------------------------------------
        "claude-codex")
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[opus]}"
            export MODEL_ORCHESTRATOR_FALLBACK="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ANALYST="${CLAUDE_MODELS[sonnet]}"      # Claude Sonnet for analysis
            export MODEL_ANALYST_FALLBACK="${CLAUDE_MODELS[haiku]}"
            export MODEL_EXECUTOR="${OPENAI_MODELS[gpt5-codex]}" # Codex for execution
            export MODEL_EXECUTOR_FALLBACK="${OPENAI_MODELS[gpt5-codex-mini]}"

            export MODEL_PROVIDER_ORCHESTRATOR="claude"
            export MODEL_PROVIDER_ANALYST="claude"
            export MODEL_PROVIDER_EXECUTOR="openai"
            ;;

        # --------------------------------------------------------
        # Full Multi-Agent: Optimal model for each role
        # --------------------------------------------------------
        "full-multiagent")
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[opus]}"
            export MODEL_ORCHESTRATOR_FALLBACK="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ANALYST="${GEMINI_MODELS[3-pro]}"       # Gemini Pro for deep analysis
            export MODEL_ANALYST_FALLBACK="${GEMINI_MODELS[3-flash]}"
            export MODEL_EXECUTOR="${OPENAI_MODELS[gpt5-codex]}" # Codex for execution
            export MODEL_EXECUTOR_FALLBACK="${OPENAI_MODELS[gpt5-codex-mini]}"

            export MODEL_PROVIDER_ORCHESTRATOR="claude"
            export MODEL_PROVIDER_ANALYST="gemini"
            export MODEL_PROVIDER_EXECUTOR="openai"
            ;;

        # --------------------------------------------------------
        # Gemini-Only: Use Gemini models for all roles
        # --------------------------------------------------------
        "gemini-only")
            export MODEL_ORCHESTRATOR="${GEMINI_MODELS[3-pro]}"
            export MODEL_ORCHESTRATOR_FALLBACK="${GEMINI_MODELS[3-flash]}"
            export MODEL_ANALYST="${GEMINI_MODELS[3-pro]}"
            export MODEL_ANALYST_FALLBACK="${GEMINI_MODELS[3-flash]}"
            export MODEL_EXECUTOR="${GEMINI_MODELS[3-flash]}"
            export MODEL_EXECUTOR_FALLBACK="${GEMINI_MODELS[2.5-flash]}"

            export MODEL_PROVIDER_ORCHESTRATOR="gemini"
            export MODEL_PROVIDER_ANALYST="gemini"
            export MODEL_PROVIDER_EXECUTOR="gemini"
            ;;

        # --------------------------------------------------------
        # Standalone/Default: Basic configuration
        # --------------------------------------------------------
        *)
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ORCHESTRATOR_FALLBACK="${CLAUDE_MODELS[haiku]}"
            export MODEL_ANALYST="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ANALYST_FALLBACK="${CLAUDE_MODELS[haiku]}"
            export MODEL_EXECUTOR="${CLAUDE_MODELS[haiku]}"
            export MODEL_EXECUTOR_FALLBACK="${CLAUDE_MODELS[haiku]}"

            export MODEL_PROVIDER_ORCHESTRATOR="claude"
            export MODEL_PROVIDER_ANALYST="claude"
            export MODEL_PROVIDER_EXECUTOR="claude"
            ;;
    esac
}

# ============================================================
# Performance Presets
# ============================================================

set_performance_preset() {
    local preset="$1"

    case "$preset" in
        "high-performance")
            # Maximum capability, higher cost
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[opus]}"
            export MODEL_ANALYST="${GEMINI_MODELS[3-pro]}"
            export MODEL_EXECUTOR="${OPENAI_MODELS[gpt5-codex]}"
            ;;
        "balanced")
            # Good performance, moderate cost
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[sonnet]}"
            export MODEL_ANALYST="${GEMINI_MODELS[3-flash]}"
            export MODEL_EXECUTOR="${OPENAI_MODELS[gpt5-codex-mini]}"
            ;;
        "cost-effective")
            # Fast and cheap
            export MODEL_ORCHESTRATOR="${CLAUDE_MODELS[haiku]}"
            export MODEL_ANALYST="${GEMINI_MODELS[3-flash]}"
            export MODEL_EXECUTOR="${OPENAI_MODELS[gpt4.1-mini]}"
            ;;
    esac
}

# ============================================================
# Helper Functions
# ============================================================

# Get model for specific role
get_model() {
    local role="$1"
    local use_fallback="${2:-false}"

    case "$role" in
        "orchestrator")
            [ "$use_fallback" = "true" ] && echo "$MODEL_ORCHESTRATOR_FALLBACK" || echo "$MODEL_ORCHESTRATOR"
            ;;
        "analyst")
            [ "$use_fallback" = "true" ] && echo "$MODEL_ANALYST_FALLBACK" || echo "$MODEL_ANALYST"
            ;;
        "executor")
            [ "$use_fallback" = "true" ] && echo "$MODEL_EXECUTOR_FALLBACK" || echo "$MODEL_EXECUTOR"
            ;;
    esac
}

# Get provider for specific role
get_provider() {
    local role="$1"

    case "$role" in
        "orchestrator") echo "$MODEL_PROVIDER_ORCHESTRATOR" ;;
        "analyst") echo "$MODEL_PROVIDER_ANALYST" ;;
        "executor") echo "$MODEL_PROVIDER_EXECUTOR" ;;
    esac
}

# Print current model configuration
print_model_config() {
    echo "═══════════════════════════════════════════════"
    echo "Multi-Agent Model Configuration"
    echo "═══════════════════════════════════════════════"
    echo ""
    echo "Workflow Type: $MCP_WORKFLOW_TYPE"
    echo ""
    echo "┌─────────────┬──────────┬─────────────────────────────┐"
    echo "│ Role        │ Provider │ Model                       │"
    echo "├─────────────┼──────────┼─────────────────────────────┤"
    printf "│ Orchestrator│ %-8s │ %-27s │\n" "$MODEL_PROVIDER_ORCHESTRATOR" "$MODEL_ORCHESTRATOR"
    printf "│ Analyst     │ %-8s │ %-27s │\n" "$MODEL_PROVIDER_ANALYST" "$MODEL_ANALYST"
    printf "│ Executor    │ %-8s │ %-27s │\n" "$MODEL_PROVIDER_EXECUTOR" "$MODEL_EXECUTOR"
    echo "└─────────────┴──────────┴─────────────────────────────┘"
    echo ""
    echo "Fallback Models:"
    echo "  Orchestrator: $MODEL_ORCHESTRATOR_FALLBACK"
    echo "  Analyst:      $MODEL_ANALYST_FALLBACK"
    echo "  Executor:     $MODEL_EXECUTOR_FALLBACK"
    echo ""
}

# ============================================================
# MCP Tool Integration
# ============================================================

# Call appropriate model based on role
mcp_call() {
    local role="$1"
    shift
    local prompt="$*"

    local model=$(get_model "$role")
    local provider=$(get_provider "$role")

    case "$provider" in
        "claude")
            # Claude Code uses Task tool with model parameter
            echo "Claude: $model"
            echo "Prompt: $prompt"
            ;;
        "gemini")
            # Use gemini-cli MCP tool
            echo "ask-gemini with model: $model"
            echo "Prompt: $prompt"
            ;;
        "openai")
            # Use codex-cli MCP tool
            echo "shell (codex) with model: $model"
            echo "Command: $prompt"
            ;;
    esac
}

# ============================================================
# Claude Code Task Tool Model Selection
# ============================================================

# Get Claude model for Task tool
get_claude_task_model() {
    local role="$1"
    local performance="${2:-balanced}"

    case "$performance" in
        "high")
            echo "opus"
            ;;
        "balanced")
            case "$role" in
                "orchestrator") echo "sonnet" ;;
                "analyst") echo "sonnet" ;;
                "executor") echo "haiku" ;;
                *) echo "sonnet" ;;
            esac
            ;;
        "fast")
            echo "haiku"
            ;;
    esac
}

# ============================================================
# Auto-Initialize
# ============================================================

# Initialize models based on workflow type
if [ -n "$MCP_WORKFLOW_TYPE" ]; then
    configure_models_for_workflow "$MCP_WORKFLOW_TYPE"
fi

# Export functions
export -f get_model get_provider print_model_config mcp_call get_claude_task_model 2>/dev/null || true
